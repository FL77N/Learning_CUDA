在前面我们可以了解到，由于GPU硬件存在一级缓存，所以非对齐的内存访问对于性能影响较小。但是CUDA实现的算法和程序中，我们无法避免非合并访问，我们可以通过使用共享内存来提高全局内存合并访问的可能。

# 5.1 CUDA共享内存概述
GPU有两种类型的内存
- 板载内存（如全局内存，有较高的延时）
- 片上内存（如共享内存，延迟低，带宽高）

共享内存的用途一般有：
1. 块内线程通信的通道
2. 用于全局内存数据的可编程管理的缓存
3. 高速暂存存储器，用于转换数据以优化全局内存访问模式

## 5.1.1 共享内存
物理上，每个SM都有一个小的低延迟内存池，这个内存池被当前正在该SM上执行的线程块的线程所共享。

每个线程块开始执行时，会分配其一定数量的共享内存。共享内存的地址空间被线程块的所有线程共享，与所在线程块具有相同的**生命周期**，理想情况下，每个被线程束共享内存访问的请求在一个事务中完成，最差情况就是拆成32个事务顺序执行。

共享内存被SM中的所有常驻线程块划分，因此，共享内存是限制设备并行性的关键资源。**一个核函数使用的共享内存越多，处于并发活跃状态的线程块越少**

## 5.1.2 共享内存分配
共享内存变量用`__shared__`修饰符声明

共享内存数组可以动态声明，注意只能**动态声明一维数组**
```cpp
extern __shared__ int tile[];
```
启动核函数时，需要将所需共享内存大小作为括号的第三个参数
```cpp
kernel<<<grid, block, isize*sizeof(int)>>>;
```

## 5.1.3 共享内存存储体和访问模式
### 5.1.3.1 内存存储体
共享内存被分为32（一个线程束所含的线程数量）个同样大小的内存模型，被称为**存储体**，可被同时访问。
如果通过线程束发布共享内存加载或存储操作，且在每个存储体上只访问不多于1个的内存地址，那么该操作可由一个内存事务完成。
### 5.1.3.2 存储体冲突
在共享内存中，**多个地址请求落在相同的内存存储体中**，就会发生存储体冲突

线程束发出共享内存请求时，有3种典型的模式：
1. 并行访问：多个地址访问多个存储体
2. 串行访问：多个地址访问单个存储体
3. 广播访问：单一地址读取单一存储体

并行访问较为常见，最佳情况是，**每个地址都位于一个单独的存储体，执行无冲突的共享内存访问**

串行访问时最坏的情况，**线程束中32个线程访问同一存储体中不同的内存地址**，需要32个内存事务

广播访问下，线程束所有线程读取同一存储体下相同的地址。**虽然只需要一个内存事务，但是每次只有一小部分字节被读取，带宽利用率很差**

### 5.1.3.3 访问模式
存储体宽度规定了共享内存地址与共享内存存储体的关系。宽度随设备计算能力的不同而变化。

存储体索引计算方式（以存储体宽度为4为例子）
```
存储体索引 = (字节地址 / 4字节每存储体) % 32存储体
```

### 5.1.3.4 内存填充
我们可以通过内存填充，改变**字到存储体的映射**，进而避免存储体的冲突

用于填充的内存不能用于数据存储，其作用是**移动数据元素**。因此带来的缺点是线程块可用的总共享内存数量减少，还需要重新计算数组索引，以访问正确的元素。

### 5.1.3.5 访问模式配置
具体可参考书181页

更改共享内存存储体的大小不会增加共享内存的使用量。一个大的存储体可能为共享内存访问产生更高的带宽，但是可能会导致更多的存储体冲突。

### 5.1.4 配置共享内存量
每个SM都有64KB的片上内存，**共享内存和一级缓存共享该硬件资源**
我们可以
- 按设备进行配置
- 按核函数进行配置

我们可以用下面的运行时函数拟合对核函数配置一级缓存和共享内存大小
```cpp
cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);
```
cacheConfig指明划分策略，包括如下： 
```cpp
cudaFuncCachePreferNone 表示默认配置
cudaFuncCachePreferShared 倾向分配48KB的shared memory和16KB的L1缓存
cudaFuncCachePreferL1 倾向分配48KB的L1缓存和16KB的shared memory
cudaFuncCachePreferEqual 倾向分配32KB的L1缓存和32KB的shared memory
```
CUDA运行时会尽可能使用请求设备的片上内存，如果需要执行一个核函数，**可自由地选择不同的配置**，参考以下CUDA运行时函数
```cpp
cudaError_t cudaFuncSetCacheConfig(const void* func, enum cudaFuncCache cacheConfig);
```

#### Tips 一级缓存与共享内存的区别
1. 共享内存通过32个存储体访问，一级缓存是通过缓存行进行访问
2. 共享内存对存储内容和存放位置有绝对的控制权，一级缓存的数据删除工作是由硬件完成的

## 5.1.5 同步
同步的两个基本方法
- 障碍：所有调用的线程等待其余调用的线程到达障碍点
- 内存栅栏：所有调用的线程必须等到全部内存修改对其余调用线程可见时才能继续执行

### 5.1.5.1 弱排序内存模型
GPU线程在不同内存中，写入数据的顺序，不一定和这些数据在源代码中访问的顺序相同。一个线程的写入顺序对其他线程可见时，可能和写操作被执行的实际顺序不一致。

为了强制程序以一个确切的顺序执行，我们需要插入内存栅栏和障碍

### 5.1.5.2 显示障碍
通过在代码中增加 `__syncthreads();`

需要注意的是在条件代码中使用`__syncthreads()`。如果只在某条件下执行，很可能让线程执行一直挂起。你需要对整个线程块的所有线程生效。一个反例是
```cpp
if(threadId % 2 == 0){
    __syncthreads();
}else{
    __syncthreads();
}
```

### 5.1.5.3 内存栅栏
内存栅栏确保栅栏前的任何内存写操作对栅栏后的其他线程都是可见的。可分为块，网格，系统的栅栏。

- `void __threadfence_block();`是线程块的栅栏，保证了栅栏前被调用和线程产生的对共享内存和全局内存的所有写操作对栅栏后同一块中的其他线程都是可见的。
- `void __threadfence();`创建网格级的栅栏，挂起调用的线程，直到全局内存中的所有写操作对相同网格内的所有线程是可见的。
- `void __threadfence_system();`创建跨系统的栅栏，挂起调用的线程，确保该线程对全局内存，锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程是可见的。

### 5.1.5.4 Volatile修饰符
修饰的变量可以防止编译器优化，就不会放置到缓存到寄存器或本地内存中。

# 5.2 共享内存的数据布局
## 5.2.1 方形共享内存
假设共享内存是一个方形数组，即
```cpp
__shared__ int tile[N][N];
```
现在我们以二维线程块去访问，方式有两种
```cpp
tile[threadIdx.y][threadIdx.x]; // 方式1
tile[threadIdx.x][threadIdx.y]; // 方式2
```
由于线程束中线程可由连续的threadIdx.x来确定。所以我们以threadIdx.x作为列索引，以访问不同的独立存储体，此时效率最高（即方式1）

### 5.2.1.1 行主序访问和列主序访问
考虑网格由1个二维线程块，每个行为都包含32个可用线程

我们执行两个操作
1. 将全局线程索引按行主序写入到一个二维共享内存数组中
2. 从共享内存中按行主序读取这些值，并存储到全局内存中

使用线程同步，保证所有线程都完成写入到共享内存数组任务后，再赋值

上面操作的核函数如下
```cpp
__global__ void setRowReadRow(int *out){
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
    tile[threadIdx.y][threadIdx.x] = idx; 

    __syncthreads();

    out[idx] = tile[threadIdx.y][threadIdx.x];
}
```
当我们把threadIdx.y和threadIdx.x对换，就得到列主序写和读取
用
```shell
nvcc checkSmemSquare.cu -o checkSmemSquare
nvprof ./checkSmemSquare
```
可以看到行主序的写和读取，速度最快（参考上面的分析）

### 5.2.1.2 行主序写和列主序读
我们只需在给输出数组赋值，交换索引即可

